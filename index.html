<!DOCTYPE HTML>
<html>
<head><title>Hello, World!</title>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<link rel="stylesheet" media="all" href="/ace/css/docco.css">
<link rel="stylesheet" media="all" href="/ace/css/markdown.css">
</head>
<body>
<div class="container"><div id="container">
<div id="markdown">

<div class="docs"><h1>Proof &#x2713; <a href="http://travis-ci.org/bigeasy/proof"><img src="https://secure.travis-ci.org/bigeasy/proof.png?branch=master" alt="Build Status" title="" /></a></h1>

<p>A test non-framework for CoffeeScript and Streamlined CoffeeScript.</p>

<h2>Philosophy</h2>

<p>Proof is a UNIX way test non-framework for the mightily lazy programmer.</p>

<p>In Proof, a <strong>unit test</strong> is a <strong>program</strong>. It does not need a runner to run. A
program emits minimal <code>Perl</code> <code>Test::Harness</code> output. Failed assertions appear as
comments in the file output.</p>

<p><strong>You write your quick and dirty diagnostics to standard error.</strong> The test
runner will hide it from you during normal test runs. If there is a test failure
you can run the test program directly to see the error spew.</p>

<p>The Proof test runner execute test programs as child processes. <strong>If a test fails,
even catastrophically, the test runner tests on.</strong></p>

<p><strong>Let the operating system do set up and tear down.</strong> When a test process exits,
even when it fails catastrophically, resources are freed for the next test
process. The test runner does not load or evaluate JavaScript, set tests up up
or tear tests down. Why count on what amounts to a fragile program loader, when
you've got a full blown operating system at your disposal?</p>

<p>When there is housekeeping to be done, databases to be reset, temporary files to
be deleted, we still don't clean up after ourselves. <strong>We clean up before
oursleves.</strong> You use Proof harnesses to clean up after the last test process at
the start of the next test process, when everything is stable.</p>

<p>With this in place, you are encouraged to <strong>be a slob</strong> in your test code.  Each
test is a short lived process, so feel free to suck up memory, leave file
handles open, and leave sockets open. The operating system knows how to close
them when your program exits. It won't affect the test runner, or test
performance. </p>

<p>Well, you'll probably always <strong>be a meticulous programmer</strong>, who would never
leave a file handle open, but still; you don't have to develop a strategy for
error handling in code that is supposed to exercise edge cases. You don't have
to <em>try</em> to <em>catch</em> the crazy monkey bananas thrown from code in development.
You have a simple, universal strategy that works for normal operation, all tests
passed, as well at the who-would-ever-have-imagined-that failures.</p>

<p>And that's not all. </p>

<p>Proof is convention over configuration until configuration is zero. Programs are
organized into directories, which act as suites. The test runner will run suites
in parallel, one test at a time from each suite. <strong>You don't have to think about
parallel to get parallel.</strong> Your operating system does parallel for you just
fine, so we use the operating system.</p>

<p>Proof is a <strong><em>parallel</em></strong> test runner, with a <strong><em>terse</em></strong> syntax and
<strong><em>generated</em></strong> scaffolding, that runs <strong><em>tests that are programs</em></strong>, and can
<strong><em>handle almost any exception</em></strong> and keep running tests.</p>

<h3>Install</h3>

<p>Via NPM.</p></div>

<div class="docs"><pre>npm install proof</pre></div>

<div class="docs"><p>To test via NPM, by extension <a href="http://travis-ci.org/">Travis CI</a>, create a
<code>package.json</code> for your project that includes the following properties.</p></div>

<div class="docs"><div class="highlight"><pre><span class="p">{</span>   <span class="nt">&quot;name&quot;</span><span class="p">:</span>             <span class="s2">&quot;fibonacci&quot;</span>
<span class="p">,</span>   <span class="nt">&quot;version&quot;</span><span class="p">:</span>          <span class="s2">&quot;1.0.3&quot;</span>
<span class="p">,</span>   <span class="nt">&quot;author&quot;</span><span class="p">:</span>           <span class="s2">&quot;Alan Gutierrez&quot;</span>
<span class="p">,</span>   <span class="nt">&quot;directories&quot;</span><span class="p">:</span>      <span class="p">{</span> <span class="nt">&quot;lib&quot;</span> <span class="p">:</span> <span class="s2">&quot;./lib&quot;</span> <span class="p">}</span>
<span class="p">,</span>   <span class="nt">&quot;devDependencies&quot;</span><span class="p">:</span>  <span class="p">{</span> <span class="nt">&quot;proof&quot;</span><span class="p">:</span> <span class="s2">&quot;&gt;=0.0.1&quot;</span> <span class="p">}</span>
<span class="p">,</span>   <span class="nt">&quot;scripts&quot;</span><span class="p">:</span>          <span class="p">{</span> <span class="nt">&quot;test&quot;</span><span class="p">:</span> <span class="s2">&quot;proof t/*/*.t&quot;</span> <span class="p">}</span>
<span class="p">,</span>   <span class="nt">&quot;main&quot;</span><span class="p">:</span>             <span class="s2">&quot;./lib/fibonacci&quot;</span>
<span class="p">}</span>

</pre></div></div>

<div class="docs"><p>Now you can run <code>npm test</code> to test your project.</p>

<h3>Every Test is a Program</h3>

<p>Every test is a program. Add a shebang line and make the file executable. </p>

<p>Minimal unit test.</p></div>

<div class="docs"><div class="highlight"><pre><span class="c1">#!/usr/bin/env coffee</span>
<span class="nx">require</span><span class="p">(</span><span class="s">&quot;proof&quot;</span><span class="p">)</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-&gt;</span> <span class="nx">@ok</span> <span class="kc">true</span><span class="p">,</span> <span class="s">&quot;true is true&quot;</span></pre></div></div>

<div class="docs"><p>The first argument to test is the number of tests to expect. If to many
or too few tests are run, the test runner will detect it and report it.</p>

<p>The call to <code>require("proof")</code> returns a function. You can call
it immediately. That makes your test preamble quick and to the point.</p>

<p>This is analogous to the above.</p></div>

<div class="docs"><pre>test = require "proof"

test 1, -&gt; @ok true, "true is true"</pre></div>

<div class="docs"><p>Here's a test with two assertions.</p></div>

<div class="docs"><div class="highlight"><pre>
<span class="c1">#!/usr/bin/env coffee</span>
<span class="nx">require</span><span class="p">(</span><span class="s">&quot;proof&quot;</span><span class="p">)</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-&gt;</span>
  <span class="nx">@ok</span> <span class="kc">true</span><span class="p">,</span> <span class="s">&quot;true is true&quot;</span>
  <span class="nx">@equal</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s">&quot;test addition&quot;</span></pre></div></div>

<div class="docs"><p>You can see that the second argument to <code>test</code> is your program. All of the
assertions in <a href="http://nodejs.org/api/assert.html"><code>require("assert")</code></a> are
available to your test function. They are bound to <code>this</code> so you can get to them
using the <code>@</code> operator in CoffeeScript.</p>

<h3>Streamline Auto-Detected</h3>

<p>Minimal streamlined unit test. Simply add a callback parameter to your callback
and your test is called asynchronously.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
fs   = require "fs"
require("proof") 1, (_) -&gt;
    found = /test/.test(fs.readFile(__filename, "utf8", _))
    @ok found, "found the word test"</pre></div>

<div class="docs"><p>When you give a test a callback with a single parameter, it calls that function
with a <code>function (error) {}</code>. This is the callback function is required by
Streamline.js.</p>

<p>No shims for Streamline.js code. Proof is Streamline.js friendly.</p>

<h3>Create More Tests More Frequently With Harnesses</h3>

<p>With an Proof harness you can start a test in as little as two lines of code.</p>

<p>Write a harness that does the setup for a test.  It will load the libraries
necessary to write a test against the subsystem of your project.</p>

<p>You add a shebang line here, because <code>proof</code> will run it for introspection when
it generates new tests. See the test generation section below.</p></div>

<div class="docs"><pre>#!/usr/bin/env coffee
context = {}
context.example = { firstName: "Alan", lastName: "Gutierrez" }
context.model = require("../../lib/model")
module.exports = require("proof") context</pre></div>

<div class="docs"><p>Now you can write tests with a mere two lines of preamble. The common setup for
your test suite is in your harness.</p></div>

<div class="docs"><pre>#!/usr/bin/env coffee
require("./harness") 2, ({ example, model }) -&gt;
  @equal model.fullName(exmaple), "Alan Gutierrez", "full name"
  @equal model.lastNameFirst(exmaple), "Gutierrez, Alan", "last name first"</pre></div>

<div class="docs"><h3>Auto-Generate Test Skeletons From Harnesses</h3>

<p>Once you have a harness, you can use <code>proof create</code> to generate tests based on
your harness.</p></div>

<div class="docs"><pre>$ proof create t/model/formats.t.coffee model example
$</pre></div>

<div class="docs"><p>You'll have a new test harness. The execute bit is set. It is ready to go.</p></div>

<div class="docs"><pre>#!/usr/bin/env coffee
require("./harness") 0, ({ example, model }) -&gt;

  # Here be dragons.</pre></div>

<div class="docs"><p>Now you can write your test.</p>

<p>Proof assumes that you've created a harness in the target directory named
<code>harness.js</code>, <code>harness._js</code>, <code>harness.coffee</code> or `harness._coffee'. You might
decide to have different name or path for your harness, or you might want to use
streamline, or specify a starting number of tests. Just type it out and proof
will figure it out.</p></div>

<div class="docs"><pre>$ proof create t/model/formats.t._coffee 2 db ./db-harness _
$</pre></div>

<div class="docs"><p>Would generate.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
require("./harness") 2, ({ db }, _) -&gt;

  # Here be dragons.</pre></div>

<div class="docs"><p>Check the docs for details.</p>

<h3>Asynchronous Harnesses</h3>

<p>Some setup will require asynchronous calls. Database connections are a common
case. You can create asynchrous harnesses by providing a callback function
instead of an object to the require method in your harness.</p>

<p>The callback function will itself get a callback that is used to return an
object that is given to the test program.</p>

<p>You'll note that, if you add a member to the object named <code>$teardown</code> that has a
function value, that function will be called at teardown time.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
mysql   = require "mysql"
fs      = require "fs"
module exports = require("proof") (callback) -&gt;
  fs.readFile "./configuration.json", "utf8", (error, file) -&gt;
    if error
      callback error
    else
      mysql = new mysql.Database(JSON.stringify file)
      mysql.connect (error, connection) -&gt;
        if error
          callback error
        else
          callback null, { connection }</pre></div>

<div class="docs"><p>Or streamlined.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
mysql   = require "mysql"
fs      = require "fs"
module exports = require("proof") (_) -&gt;
  file = fs.readFile "./configuration.json", "utf8", _
  mysql = new mysql.Database(JSON.stringify file)
  conneciton = mysql.connect _
  { connection }</pre></div>

<div class="docs"><p>The test itself is no more complicated.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
require("./harness") 1, ({ connection }, _) -&gt;
  results = connection.sql("SELECT COUNT(*) AS num FROM Employee", _)
  @equal 12, results[0].num, "employee count"
  connection.close()</pre></div>

<div class="docs"><p>Note that, you can use asynchronous harnesses with synchronous tests, and create
asynchronous tests from synchronous harnesses.</p>

<h3>Exception Handling</h3>

<p>When a test is healthy, it is healthy for the test to release resources. When a
test fails catastrophically, Proof lets the operating system to hard work of
releasing system resources, such as memory, sockets and file handles.</p>

<p>Here's a test that opens a file handle, then closes it like a good citizen.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
require("./harness") 1, ({ fs, tmp }, _) -&gt;
  fs.open(__filename, "r", _)

  buffer = new Buffer(2)
  fs.read(fd, buffer, 0, buffer.length, 0, _)
  @equal buffer.readInt16BE(0), 0x2321, "shebang magic number"

  fs.close(fd, _)</pre></div>

<div class="docs"><p>But, what if there is a catastrophic error? Let's say that in the code above,
the <code>Buffer</code> cannot be allocated because the system is out of memory. What
happens? An exception is thrown, the process exits, and the operating system
closes the file handle.</p>

<p>Of course, we could register an error handler to run at the last minute, or we
could create a towering pyramid of try/catch blocks, but why bother? Let the
operating system handle your exceptions in your test code. Much easier than
trying to account for every possible error in code that is under active
development.</p>

<h3>Housekeeping</h3>

<p>Rather than cleaning up after ourselves, we cleanup before ourselves. We create
a harness that cleanups any mess form a previously failed test.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
require("./harness") 1, ({ fs, spawn, cwd }, _) -&gt;
  program = "#{cwd}/example.sh"
  fs.writeFile(program, "utf8", "#!/bin/bash\nexit 1\n", _)

  try
    exec "#{cwd}/example.sh", _
  catch e
    @equal e.code, 1, "failed"</pre></div>

<div class="docs"><p>In the test above, we create a bash program to to test that error codes work
correctly. If no exception is thrown, the test runner will report that a test
was missed.</p>

<p>This leaves a file lying around. How do we clean it up? With a harness.</p>

<p>Here's a streamlined harness for our function.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
mysql   = require "mysql"
fs      = require "fs"
module.exports = require("proof") -&gt;
  cwd = "#{__dirname}"
  $cleanup = (_) -&gt;
    try
      fs.unlink "#{cwd}/example.sh", _
    catch e
      throw e if e.code isnt "ENOENT" 
  { cwd, $cleanup }</pre></div>

<div class="docs"><p>As you can see, your cleanup function can use Streamline.js, even if your
harness function does not.</p>

<p>We run your clean up function at the start of a test, then again at the end of
the test if the test exits normally. Thus, even if the test fails, everything
will be clean for the next test using the same harness.</p>

<p>Your <code>$cleanup</code> function must be <strong>idempotent</strong>, as the kids like to say. You
must be able to run it over and over again and get the same results. If your
cleanup function deletes a temporary file, it can't complain if the temporary
file has already been deleted.</p>

<p>What if you're <code>$cleanup</code> function deletes something you need? Then you provide
a <code>$setup</code> function. Before your test is run, <code>$cleanup</code> is run to clear out the
remains of any failed tests, then <code>$setup</code> is run. After your test is run
<code>$cleanup</code> is run, and everything goes back to the way it was.</p></div>

<div class="docs"><pre>#!/usr/bin/env _coffee
mysql   = require "mysql"
fs      = require "fs"
module.exports = require("proof") -&gt;
  tmp = "#{__dirname}/tmp"
  $setup = (_) -&gt;
    fs.mkdir tmp, 0755, _
  $cleanup = (_) -&gt;
    try
      for file in fs.readdir tmp, _
        fs.unlink file, _
      fs.unlink tmp, _
    catch e
      throw e if e.code isnt "ENOENT" 
  { tmp, $setup, $cleanup }</pre></div>

<div class="docs"><p>Now we can use a temporary directory to create our bash program.</p></div>

<div class="docs"><div class="highlight"><pre>
<span class="c1">#!/usr/bin/env _coffee</span>
<span class="nx">require</span><span class="p">(</span><span class="s">&quot;./harness&quot;</span><span class="p">)</span> <span class="mi">1</span><span class="p">,</span> <span class="nf">({ fs, spawn, tmp }, _) -&gt;</span>
  <span class="nv">program = </span><span class="s">&quot;</span><span class="si">#{</span><span class="nx">tmp</span><span class="si">}</span><span class="s">/example.sh&quot;</span>
  <span class="nx">fs</span><span class="p">.</span><span class="nx">writeFile</span><span class="p">(</span><span class="nx">program</span><span class="p">,</span> <span class="s">&quot;utf8&quot;</span><span class="p">,</span> <span class="s">&quot;</span><span class="err">#</span><span class="s">!/bin/bash\nexit 1\n&quot;</span><span class="p">,</span> <span class="nx">_</span><span class="p">)</span>

  <span class="k">try</span>
    <span class="nx">exec</span> <span class="s">&quot;</span><span class="si">#{</span><span class="nx">tmp</span><span class="si">}</span><span class="s">/example.sh&quot;</span><span class="p">,</span> <span class="nx">_</span>
  <span class="k">catch</span> <span class="nx">e</span>
    <span class="nx">@equal</span> <span class="nx">e</span><span class="p">.</span><span class="nx">code</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="s">&quot;failed&quot;</span>

</pre></div></div>

<div class="docs"><h3>Running Tests</h3>

<p>A test is a program. You can run a test to see its output.</p></div>

<div class="docs"><pre>$ t/logic/minimal.t
1..2
ok 1 - true is true
ok 2 - test arithmetic
$</pre></div>

<div class="docs"><p>By immutable convention, tests are grouped together by directory. The tests
within a directory are considered a suite of tests. Test suites are generally
kept in a directory <code>t</code> off the root of the project.</p></div>

<div class="docs"><pre>$ find t
t/logic/minimal.t
t/regex/minimal.t
t/regex/complex.t
$</pre></div>

<div class="docs"><p>You can run a test with the proof test runner to get gaudy console output with
colors and non-ASCII characters (approximated below).</p>

<pre>
$ proof t/logic/minimal.t.coffee
 &#x2713; t/logic/minimal.t.coffee ......................... (2/2)  .230 Success
$
</pre>

<p>Each test you pass to the test runner is will be run by the test runner. Tests
in separate suites are run in parallel.</p>

<pre>
$ proof t/logic/minimal.t.coffee t/regex/minimal.t.coffee
 &#x2713; t/logic/minimal.t.coffee ......................... (2/2)  .230 Success
 &#x2713; t/regex/minimal.t.coffee ......................... (2/2)  .331 Success
$
</pre>

<h3>Tests Run in Parallel</h3>

<p>As above.</p>

<pre>
$ proof t/logic/minimal.t.coffee t/regex/minimal.t.coffee t/regex/complex.t.coffee
 &#x2713; t/logic/minimal.t.coffee ......................... (2/2)  .230 Success
 &#x2713; t/regex/minimal.t.coffee ......................... (2/2)  .331 Success
 &#x2713; t/regex/complex.t.coffee ......................... (2/2) 1.045 Success
$
</pre>

<p>Because each test is a program, parallelism is simply a matter of running more
than one test program at once. The default mode of the proof runner is to run four
test programs a time.</p>

<p>The runner will run a test program from each suite, for up to four programs
running at once. When a suite it complete, it moves onto the next one.</p>

<p>Tests within suites are run one after another, in the order in which they were
specified on the command line.</p>

<p>If you design your tests so that they can run in any order, then you can run an
entire suite of tests with globbing.</p>

<pre>
$ proof t/*/*.t.coffee
 &#x2713; t/logic/minimal.t.coffee ......................... (2/2)  .230 Success
 &#x2713; t/regex/minimal.t.coffee ......................... (2/2)  .331 Success
 &#x2713; t/regex/complex.t.coffee ......................... (2/2) 1.045 Success
$
</pre>

<p>Generally avoid making tests depend on being run in a specific order. That way
you can get parallelism easily.</p>

<p>Suites run in parallel. You can group your tests however you like, by feature,
subsystem, stages of workflow.</p>

<p>Make sure they can run in parallel though. You may have a single MySQL database
to use for testing. You'll have to  group all your MySQL tests in a suite, you
can be sure that they won't stomp on each other. If your application supports
either MySQL or PostgreSQL, you could run those tests in parallel.</p>

<p>If every test expects to hit a MySQL database, then create a separate MySQL
database for each suite. Not a big deal, really, and then you have your tests
running in parallel.</p>

<h3>Continuous Integration With Travis CI</h3>

<p>For an example of Travis CI output, you can look at the <a href="http://travis-ci.org/#!/bigeasy/ace">output from Proof
itself</a>.</p>

<p>With a minimal <code>.travis.yml</code> and Proof will work with Travis CI.</p></div>

<div class="docs"><pre>language: node_js

node_js:
  - 0.6

before_install:
  - npm install --dev
  - git submodule init &amp;&amp; git submodule update</pre></div>

<div class="docs"><p>However, <code>npm install --dev</code> will recursively install development dependencies,
bringing in all of CoffeeScript, which is not necessary. Explicitly installing
your development dependencies makes your Travis CI output much less verbose.</p></div>

<div class="docs"><pre>language: node_js

node_js:
  - 0.6

before_install:
  - npm install
  - npm install proof coffee-script streamline
  - git submodule init &amp;&amp; git submodule update</pre></div>

<div class="docs"><p>If you are not using Streamline.js exclude <code>streamline</code>. If you are not using
CoffeeScript exclude <code>coffee-script</code>. Add additional development dependencies as
to suit your project's needs.</p>

<h2>Why I Wrote My Own (Non-)Framework</h2>

<p>This is my test framework. There are many like it, but this one is mine.</p>

<p>The catalyist was Streamline.js. The frameworks that exist generally group tests
into a object or the functional programming equavalent. Each test is a function.
A suite is a class or other grouping of functions.</p>

<p>The function may provide a callback, but the callback isn't in the <code>function
(error, result) {}</code> format that works with Streamline.js.</p>

<p>Of course, once I was done writing this, it occured to me that callback
signature differences are easily shimmed, but it was too late by then.</p>

<p>There is a lot of extra stuff your grarden variety test frameworks. Folks put a
lot of thought into testing, which is an area that invites over-thinking.</p>

<p>I don't want to pay for those features; compatability with CI frameworks I don't
use, compatability with IDEs I don't use, histograms that I'll never look at.</p>

<p>I've never thought to myself, boy, if only I could see my test results in XML,
JSON, RDF and YAML, then I'd really get to the bottom of this pesky bug.</p>

<p>I don't want to depend on a test runner to run my tests.  The test runner here
runs tests, it does not load them, set them up and tear them down. It just runs
them and reports on how they ran.</p>

<p>I don't want to have fiddle with a test runner to run a specific test. I want a
each test to be a a program. I want to group my tests into suites by grouping my
test programs into directories. When I want to run a specific test, I'll just
run the test program directly.</p>

<p>I'm happy to pay the extra millis to spawn a process per test, because it is so
much easier to write a test program, with a clean program state, and not have to
worry about teardown. I don't want my test runner to manage memory, file handles
and sockets. Let the operating system to do that.</p>

<p>I do want pretty green check marks. Those are <em>very</em> important to me. You'll see
that my check marks are green and they use UNICODE check marks. I splurged on
the check marks. They make me happy.</p>

<h2>Un-Filtered Blather</h2>

<ul>
<li>Deeply nested try/catch blocks.</li>
<li>Frameworks group assertions into tests and tests into suites. A suite is a
file containing test. To run a test you need to run it through the test
runner. Proof has the same grouping this too, but a suite is a directory, a
test is a program.  Thus, to run a specific test, you run that program. No
special switches to the test runner to pluck out the test you want to run.</li>
<li><em>We cleanup at setup instead of tearing down.</em> You can be loosey goosey about
resources in your tests because they are short lives programs, but if you
need to have a an empty directory, or a reset a database state, do it at test
start, and leave it a mess for the next run to cleanup. If you want to be
tidy, you can have a cleanup function that you run at the end, but run it at
the begining as well in case the last run ended early.</li>
<li>Tests are meant to exercise the demons in our software. We're not surprised
when they fail catastrohpically. Why do we try to register last minute
callbacks to do housekeeping in a process that might have just decided to
kill itself rather than go on? Why do try to run test after test in a single
long-running test runner process, when we know that an error most likely
means resources left unused, yet uncollected?</li>
<li>Do we want to manage all the failure states for code in development? A long
running test runner is a horrible model for a test framework.</li>
<li>We write tests because we know that static analysis is not enough to ensure
quality.  We write tests to exercise the demons from our software. We should
not be surprised when our tests fail in ways that we cannot imagine. We
should not feel ashamed that we could never imagine the failure states that
our tests uncover. We should be humble before the complexity of software. We
should accept that it is more than we can hold in our head at once.</li>
<li>To think otherwise is to expect oneself to be omnicient.</li>
<li>Moreover, why do you need a plan to turn off every light switch on your way
out the door when the house is burning down? Dont' worry. The fire will take
care of it for you.</li>
</ul></div>

</div>
</div>

</div>
</body>
</html>
